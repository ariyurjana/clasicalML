{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REYPoTJ1Af7u"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.datasets import load_boston\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTQ2iH-kAwbo"
   },
   "outputs": [],
   "source": [
    "X = load_boston().data\n",
    "Y = load_boston().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgUW39s6Be0U"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HNu1sXtEBBS2",
    "outputId": "a0c5d3d0-345c-4a34-8ccf-0d9f3d7d23fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.959212518860685\n",
      "{'alpha': 0.0001, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.01, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'invscaling', 'loss': 'squared_loss', 'max_iter': None, 'n_iter': None, 'n_iter_no_change': 5, 'penalty': 'l2', 'power_t': 0.25, 'random_state': None, 'shuffle': True, 'tol': None, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "clf = SGDRegressor()\n",
    "clf.fit(X, Y)\n",
    "print(mean_squared_error(Y,clf.predict(X)))\n",
    "\n",
    "print(clf.get_params(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.67371808  0.61967151 -0.24103216  0.72957934 -0.89430692  3.18483195\n",
      " -0.18434904 -2.25869644  1.09746133 -0.612922   -1.79249909  0.8783723\n",
      " -3.26719759]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time    \n",
    "import math\n",
    "import pickle \n",
    "\n",
    "\n",
    "class LinRegr:    \n",
    "    \"\"\"\n",
    "        we have two main methods SGDProcess and SGDProcess1.\n",
    "        SGDProcess first preforms linear regression and gets the weights. We will optimize these weights\n",
    "        SGDProcess1 starts with weight vector all initialized to zero\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #data\n",
    "        self.Xdata = []\n",
    "        self.ydata = []\n",
    "        self.y_hat_tst_opt = []\n",
    "        #regressor\n",
    "        self.linrgr = None\n",
    "        \n",
    "        #output beta estimates\n",
    "        self.coeff = []\n",
    "        #intercept\n",
    "        self.intcept = 0.0\n",
    "        #predictions\n",
    "        self.pred = []\n",
    "        #R squared\n",
    "        self.rsquared = 0.0\n",
    "        #mean squared error\n",
    "        self.MSE = 0.0\n",
    "        #sample points for SGD \n",
    "        self.x_sgdt = []\n",
    "        self.y_sgdt = []\n",
    "        \n",
    "        #intial weigts W0 and intercepts\n",
    "        self.w_0 = []\n",
    "        self.w_prev = []\n",
    "        self.w_next = []\n",
    "        self.w_opt = []\n",
    "        self.partial_w = []\n",
    "        \n",
    "        self.intcpt_prev = 0.0\n",
    "        self.intcpt_next = 0.0\n",
    "        self.intcpt_opt = 0.0\n",
    "        self.partial_intcpt = 0.0\n",
    "        \n",
    "        #no of iterations for SGD\n",
    "        self.num_iter = 0\n",
    "        self.num_of_pts = 0\n",
    "        \n",
    "        #learning rate\n",
    "        self.learning_rate = 0\n",
    "        \n",
    "        \n",
    "        self.x_sgdt_df = pd.DataFrame()\n",
    "        self.y_sgdt_df = pd.DataFrame()\n",
    "        self.term2= []\n",
    "        \n",
    "        \n",
    "    #constructor\n",
    "    def linearegrsn(self):\n",
    "        self.linrgr = LinearRegression(fit_intercept=True,normalize=True)\n",
    "        return self.linrgr\n",
    "    \n",
    "    #getter/setters\n",
    "    #LINEAR REGRESSOR    \n",
    "    @property\n",
    "    def linrgr(self):\n",
    "        return self._linrgr\n",
    "    \n",
    "    @linrgr.setter\n",
    "    def linrgr(self,new_linrgr):\n",
    "        self._linrgr = new_linrgr     \n",
    "        \n",
    "    @property\n",
    "    def Xdata(self):\n",
    "        return self._Xdata\n",
    "    \n",
    "    @Xdata.setter\n",
    "    def Xdata(self,new_Xdata):\n",
    "        self._Xdata = new_Xdata\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def ydata(self):\n",
    "        return self._ydata            \n",
    "    @ydata.setter\n",
    "    def ydata(self,new_ydata):\n",
    "        self._ydata = new_ydata        \n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def x_train(self):\n",
    "        return self._x_train\n",
    "    \n",
    "    @x_train.setter\n",
    "    def x_train(self,new_x_train):\n",
    "        self._x_train = new_x_train\n",
    "\n",
    "    @property\n",
    "    def x_test(self):\n",
    "        return self._x_test\n",
    "    \n",
    "    @x_test.setter\n",
    "    def x_test(self,new_x_test):\n",
    "        self._x_test = new_x_test\n",
    "        \n",
    " \n",
    "    @property\n",
    "    def y_train(self):\n",
    "        return self._y_train\n",
    "    \n",
    "    @y_train.setter\n",
    "    def y_train(self,new_y_train):\n",
    "        self._y_train = new_y_train\n",
    "\n",
    "    @property\n",
    "    def y_test(self):\n",
    "        return self._y_test\n",
    "    \n",
    "    @y_test.setter\n",
    "    def y_test(self,new_y_test):\n",
    "        self._y_test = new_y_test\n",
    "     \n",
    "    @property\n",
    "    def y_hat_tst_opt(self):\n",
    "        return self._y_hat_tst_opt\n",
    "    \n",
    "    @y_hat_tst_opt.setter\n",
    "    def y_hat_tst_opt(self,new_y_hat_tst_opt):\n",
    "        self._y_hat_tst_opt = new_y_hat_tst_opt        \n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def coeff(self):\n",
    "        return self._coeff\n",
    "    \n",
    "    @coeff.setter\n",
    "    def coeff(self,new_coef):\n",
    "        self._coeff = new_coef\n",
    "\n",
    "    @property\n",
    "    def intcept(self):\n",
    "        return self._intcept\n",
    "    \n",
    "    @intcept.setter\n",
    "    def intcept(self,new_intcept):\n",
    "        self._intcept = new_intcept\n",
    "        \n",
    "    @property\n",
    "    def pred(self):\n",
    "        return self._pred\n",
    "    \n",
    "    @pred.setter\n",
    "    def pred(self,new_pred):\n",
    "        self._pred = new_pred\n",
    "\n",
    "    @property\n",
    "    def rsquared(self):\n",
    "        return self._rsquared\n",
    "    \n",
    "    @rsquared.setter\n",
    "    def rsquared(self,new_rsquared):\n",
    "        self._rsquared = new_rsquared\n",
    "        \n",
    "    @property\n",
    "    def MSE(self):\n",
    "        return self._MSE\n",
    "    \n",
    "    @MSE.setter\n",
    "    def MSE(self,new_mse):\n",
    "        self._MSE = new_mse        \n",
    "        \n",
    "    @property\n",
    "    def w_0(self):\n",
    "        return self._w_0\n",
    "    \n",
    "    @w_0.setter\n",
    "    def w_0(self,new_w_0):\n",
    "        self._w_0 = new_w_0\n",
    "\n",
    "    @property\n",
    "    def w_prev(self):\n",
    "        return self._w_prev\n",
    "    \n",
    "    @w_prev.setter\n",
    "    def w_prev(self,new_w_prev):\n",
    "        self._w_prev = new_w_prev\n",
    "\n",
    "    @property\n",
    "    def w_next(self):\n",
    "        return self._w_next\n",
    "    \n",
    "    @w_next.setter\n",
    "    def w_next(self,new_w_next):\n",
    "        self._w_next = new_w_next\n",
    "\n",
    "    @property\n",
    "    def w_opt(self):\n",
    "        return self._w_opt\n",
    "    \n",
    "    @w_opt.setter\n",
    "    def w_opt(self,new_w_opt):\n",
    "        self._w_opt = new_w_opt\n",
    "        \n",
    "    @property\n",
    "    def partial_w(self):\n",
    "        return self._partial_w\n",
    "    \n",
    "    @partial_w.setter\n",
    "    def partial_w(self,new_partial_w):\n",
    "        self._partial_w = new_partial_w\n",
    "        \n",
    "    @property\n",
    "    def intcpt_prev(self):\n",
    "        return self._intcpt_prev\n",
    "    \n",
    "    @intcpt_prev.setter\n",
    "    def intcpt_prev(self,new_intcpt_prev):\n",
    "        self._intcpt_prev = new_intcpt_prev      \n",
    "        \n",
    "    @property\n",
    "    def intcpt_next(self):\n",
    "        return self._intcpt_next\n",
    "    \n",
    "    @intcpt_next.setter\n",
    "    def intcpt_next(self,new_intcpt_next):\n",
    "        self._intcpt_next = new_intcpt_next       \n",
    "\n",
    "    @property\n",
    "    def intcpt_opt(self):\n",
    "        return self._intcpt_opt\n",
    "    \n",
    "    @intcpt_opt.setter\n",
    "    def intcpt_opt(self,new_intcpt_opt):\n",
    "        self._intcpt_opt = new_intcpt_opt\n",
    "        \n",
    "    @property\n",
    "    def partial_intcpt(self):\n",
    "        return self._partial_intcpt\n",
    "    \n",
    "    @partial_intcpt.setter\n",
    "    def partial_intcpt(self,new_partial_intcpt):\n",
    "        self._partial_intcpt = new_partial_intcpt\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def num_iter(self):\n",
    "        return self._num_iter\n",
    "    \n",
    "    @num_iter.setter\n",
    "    def num_iter(self,new_num_iter):\n",
    "        self._num_iter = new_num_iter\n",
    "        \n",
    "    @property\n",
    "    def num_of_pts(self):\n",
    "        return self._num_of_pts\n",
    "    \n",
    "    @num_of_pts.setter\n",
    "    def num_of_pts(self,new_numpts):\n",
    "        self._num_of_pts = new_numpts\n",
    "        \n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self._learning_rate\n",
    "    \n",
    "    @learning_rate.setter\n",
    "    def learning_rate(self,new_learning_rate):\n",
    "        self._learning_rate= new_learning_rate\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def x_sgdt_df(self):\n",
    "        return self._x_sgdt_df\n",
    "    \n",
    "    @x_sgdt_df.setter\n",
    "    def x_sgdt_df(self,new_x_sgdt_df):\n",
    "        self._x_sgdt_df = new_x_sgdt_df\n",
    "        \n",
    "    @property\n",
    "    def y_sgdt_df(self):\n",
    "        return self._y_sgdt_df\n",
    "    \n",
    "    @y_sgdt_df.setter\n",
    "    def y_sgdt_df(self,new_y_sgdt_df):\n",
    "        self._y_sgdt_df = new_y_sgdt_df\n",
    "        \n",
    "    @property\n",
    "    def x_sgdt(self):\n",
    "        return self._x_sgdt\n",
    "    \n",
    "    @x_sgdt.setter\n",
    "    def x_sgdt(self,new_x_sgdt):\n",
    "        self._x_sgdt = new_x_sgdt\n",
    "        \n",
    "    @property\n",
    "    def y_sgdt(self):\n",
    "        return self._y_sgdt\n",
    "    \n",
    "    @y_sgdt.setter\n",
    "    def y_sgdt(self,new_y_sgdt):\n",
    "        self._y_sgdt = new_y_sgdt\n",
    "        \n",
    "    @property\n",
    "    def term2(self):\n",
    "        return self._term2\n",
    "    \n",
    "    @term2.setter\n",
    "    def term2(self,new_term2):\n",
    "        self._term2 = new_term2\n",
    "        \n",
    "     #load the boston dataset\n",
    "    def load_data(self):\n",
    "        self.Xdata = load_boston().data\n",
    "        self.ydata = load_boston().target \n",
    "        # split into x and y train and test\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.Xdata, self.ydata, test_size=0.33, random_state=5) \n",
    "        #standardize the x train and test data\n",
    "        scaler = StandardScaler().fit(self.x_train)\n",
    "        self.x_train = scaler.transform(self.x_train)\n",
    "        self.x_test = scaler.transform(self.x_test)\n",
    "        #we need to get random points from the data set\n",
    "        #hence the x and y data are joined together so that when we get random points we get x and also the\n",
    "        #corresponding y values\n",
    "        self.x_sgdt_df = pd.DataFrame(self.x_train)\n",
    "        self.x_sgdt_df['price'] = self.y_train\n",
    "    \n",
    "    #fit data\n",
    "    def lnrg_fitdata(self):        \n",
    "        self.linrgr = (self.linrgr).fit(self.x_train,self.y_train)\n",
    "        return self.linrgr\n",
    "    \n",
    "    #predict data\n",
    "    def lnrgr_predict(self):\n",
    "        self.pred = (self.linrgr).predict(self.x_test)\n",
    "        return (self.linrgr,self.pred)\n",
    "    \n",
    "    #r squared\n",
    "    def lnrgr_rsq(self):\n",
    "        self.rsquared = (self.linrgr).score(self.Xdata,self.ydata)\n",
    "        return self.rsquared\n",
    "    \n",
    "    #beta estimates\n",
    "    def getCoeff(self):\n",
    "        self.coeff = (self.linrgr).coef_\n",
    "        self.w_0 = (self.linrgr).coef_\n",
    "        return self.coeff\n",
    "    \n",
    "    #get intercepts\n",
    "    def getintercepts(self):\n",
    "        self.intcept = (self.linrgr).intercept_\n",
    "        return self.intcept\n",
    "    \n",
    "    #get random k points from the datset for SGD\n",
    "    def generaterandomsample(self):\n",
    "        tmp_df = (self.x_sgdt_df.sample(self.no_of_pts))\n",
    "        self.x_sgdt = tmp_df.drop('price',axis=1).values \n",
    "        self.y_sgdt = tmp_df['price'].values \n",
    "                \n",
    "    \n",
    "    def pred1(self):\n",
    "        \n",
    "        y_pred=[]        \n",
    "        \n",
    "        for i in range(self.x_test.shape[0]):\n",
    "            y=np.asscalar(np.dot(self.w_opt,self.x_test[i])+self.intcpt_opt)\n",
    "            y_pred.append(y)\n",
    "        np.array(y_pred)\n",
    "        print(mean_squared_error(self.y_test,y_pred))\n",
    "        return y_pred\n",
    "\n",
    "    # in this process we start with all weights equal to zero and move towards w-star\n",
    "    #SGDProcess_1\n",
    "    def SGDProcess_1(self,niter,learnrate,nrows):\n",
    "        #load boston data\n",
    "        self.Xdata = load_boston().data\n",
    "        self.ydata = load_boston().target \n",
    "        \n",
    "        # split into train and test\n",
    "        train_data, test_data, train_y, test_y=train_test_split(self.Xdata, self.ydata, test_size=0.33, random_state=5)\n",
    "        \n",
    "        #standard scaler used to standardize data\n",
    "        stdscaler=StandardScaler()\n",
    "        train_data=stdscaler.fit_transform(np.array(train_data))\n",
    "        test_data=stdscaler.transform(np.array(test_data))\n",
    "        \n",
    "        # join X and corresponding Y values\n",
    "        train_df = pd.DataFrame(data=train_data)\n",
    "        nfeat = len(train_df.columns)\n",
    "        train_df['Price'] = train_y\n",
    "        \n",
    "        tst_x_np = np.array(test_data)        \n",
    "        tst_y_np = np.array(test_y)\n",
    "        \n",
    "        #initialize the epoochs and learning rate\n",
    "        self.num_iter = niter\n",
    "        self.learning_rate = learnrate\n",
    "        \n",
    "        #initialize w_next and intercept next\n",
    "        self.w_next = np.zeros(shape=(1,nfeat))\n",
    "        self.intcpt_next = 0\n",
    "        \n",
    "        nolops = 1\n",
    "        \n",
    "        #start the run\n",
    "        while(nolops <= self.num_iter):\n",
    "            self.w_prev = self.w_next\n",
    "            self.intcpt_prev = self.intcpt_next\n",
    "            w_tmp = np.zeros(shape=(1,13))\n",
    "            b_tmp=0\n",
    "            \n",
    "            #nrows is  the number of samples that we need to get from the data\n",
    "            #sample returns the x and y data\n",
    "            trn_data = train_df.sample(nrows)\n",
    "            #drop the price data and get only the xdata\n",
    "            x_dt = np.array(trn_data.drop('Price',axis=1))\n",
    "            #get only the y data corresponding to the X values\n",
    "            y_dt = np.array(trn_data['Price'])\n",
    "            \n",
    "            \n",
    "            for i in range(nrows):\n",
    "                #predict the  y value\n",
    "                y_pred = np.dot(self.w_prev,x_dt[i]) + self.intcpt_next\n",
    "                # generate the weights and intercept\n",
    "                w_tmp += x_dt[i] * (y_dt[i] - y_pred)\n",
    "                b_tmp += (y_dt[i] - y_pred)\n",
    "                \n",
    "            w_tmp *= (-2/nrows)\n",
    "            b_tmp *= (-2/nrows)\n",
    "            \n",
    "            #get the update value for the weights\n",
    "            self.w_next = self.w_prev - (self.learning_rate * w_tmp)\n",
    "            self.intcpt_next = self.intcpt_prev - (self.learning_rate * b_tmp)\n",
    "            \n",
    "            self.learning_rate = self.learning_rate / pow(nolops,0.25)\n",
    "            #increment the loop\n",
    "            nolops += 1\n",
    "        \n",
    "        # using the w_start and intercept \n",
    "        #use the test data to generate the y value \n",
    "        y_pred=[]\n",
    "\n",
    "        for i in range(len(tst_x_np)):\n",
    "            y=np.asscalar(np.dot(self.w_next,tst_x_np[i])+self.intcpt_next)\n",
    "            y_pred.append(y)\n",
    "        #calculate the mean squared error and print it out\n",
    "        print('Mean Squared Error',mean_squared_error(test_y,y_pred))\n",
    "        \n",
    "\n",
    "        return self.w_next, self.intcpt_next              \n",
    " \n",
    "    #in this follwing process we run linear regression and generate weights for the given data\n",
    "    #then we use the weights as the starting point and iterate till we get w_star\n",
    "    #SGD process\n",
    "    def SGDProcess(self,niter, npts,nmfeat):\n",
    "        #initializations\n",
    "        self.num_iter = niter\n",
    "        self.no_of_pts = npts\n",
    "        num_feat = nmfeat\n",
    "        num_rows = npts\n",
    "        self.w_prev = self.w_0\n",
    "        self.intcpt_prev = self.intcept      \n",
    "        \n",
    "        nolops=1        \n",
    "        w_diff = []\n",
    "        \n",
    "        #start the run\n",
    "        while(nolops<=self.num_iter):                  \n",
    "            self.w_next = np.zeros(shape=(1,num_feat))\n",
    "            self.partial_w = np.zeros(shape=(1,num_feat))\n",
    "            self.intcpt_next = 0.0\n",
    "            #generate the random sample data points\n",
    "            self.generaterandomsample()\n",
    "            y_pred = np.zeros(num_rows)\n",
    "            x=np.array(self.x_sgdt)\n",
    "            y=np.array(self.y_sgdt)\n",
    "            \n",
    "            for i in range(num_rows):      \n",
    "                #predict the value\n",
    "                y_pred=np.dot( self.w_prev,x[i])+self.intcpt_prev                                \n",
    "                self.partial_w+=x[i] * (y[i] - y_pred)  \n",
    "                #print(type(self.partial_w),self.partial_w.shape, x.shape)\n",
    "                self.intcpt_next+=(y[i]-y_pred)\n",
    "            #print(num_rows)\n",
    "            self.partial_w *=(-2/num_rows)\n",
    "            self.intcpt_next*=(-2/num_rows)\n",
    "                        \n",
    "            #updating the weight vector            \n",
    "            self.w_next=(self.w_prev-(self.learning_rate*self.partial_w))\n",
    "            #print(type(self.w_next),self.w_next.shape)\n",
    "            self.intcpt_next=(self.intcpt_prev-(self.learning_rate*self.intcpt_next))\n",
    "            \n",
    "            #generate the MSE \n",
    "            self.w_opt = self.w_next\n",
    "            self.intcpt_opt = self.intcpt_next            \n",
    "            self.eval_yhat_opt()\n",
    "            ret_mse = self.eval_MSE()\n",
    "            \"\"\"\n",
    "            #if ret_mse[0] <= 30:\n",
    "            if self.checkallval(self.w_next,num_feat):\n",
    "                print('SOLUTION CONVERGED',t)\n",
    "                self.w_opt = self.w_next\n",
    "                self.intcpt_opt = self.intcpt_next\n",
    "                self.learning_rate = r\n",
    "                \n",
    "                break\n",
    "            else:\n",
    "                self.w_prev = self.w_next\n",
    "                self.intcpt_prev = self.intcpt_next\n",
    "                r = r /10\n",
    "                self.learning_rate = r                \n",
    "                #print('SOLUTION NOT CONVERGED \\n',t,'****',r)\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            #transfer the new weights int old weights\n",
    "            self.w_prev = self.w_next            \n",
    "            self.intcpt_prev = self.intcpt_next\n",
    "            #update the learning rate                  \n",
    "            self.learning_rate = self.learning_rate / 3\n",
    "            #print('r',r,t)\n",
    "            #increment the epoch\n",
    "            nolops+=1\n",
    "        #at the end of the loop the w_next and intercept_next are w_star and interpret_star\n",
    "        self.w_opt = self.w_next\n",
    "        self.intcpt_opt = self.intcpt_next\n",
    "        \n",
    "        \n",
    "        return [self.w_next,  self.intcpt_next, self.learning_rate]\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    def checkallval(self,wdiff,nofeat):\n",
    "        j= 0\n",
    "        k= 0\n",
    "        diff = 0\n",
    "        for i in range(0,len(wdiff)):\n",
    "            prev = np.asarray(self.w_prev[0])\n",
    "            nxt = np.asarray(self.w_next[0])\n",
    "            #print(i,self.w_next.shape,self.w_prev.shape,len(wdiff))\n",
    "            #print(type(prev[i]),prev.shape,'next', nxt[i])\n",
    "            diff = (prev[i] - nxt[i])\n",
    "            print('diff',diff)\n",
    "            if  diff <= 0.0000001:\n",
    "                #print(\"diff less than 0.00001\\n\")\n",
    "                j+=1\n",
    "            elif diff > 0.0000001:\n",
    "                #print(\"diff greater than 0.00001\\n\")\n",
    "                j-=1\n",
    "                \n",
    "        if j==nofeat:\n",
    "            print('**************',j)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "            #if wdiff[i] >0.0000001:\n",
    "                #return False\n",
    "        \n",
    "    #generate y_hat using the w_star created by the sgd Process\n",
    "    def eval_yhat_opt(self):\n",
    "        \n",
    "        num_rows = self.y_test.shape[0]        \n",
    "        self.y_hat_tst_opt = np.zeros(shape=(num_rows,1))\n",
    "        for k in range(0,num_rows):\n",
    "            #Y1 = W1*X11+w2*x12+w3*x13+w4*x14....Wn*Xn<num_feat>                \n",
    "            self.y_hat_tst_opt[k] = np.dot(self.w_opt,self.x_test[k])+self.intcpt_opt                        \n",
    "            #print(self.w_opt , self.x_test[k],self.y_hat_tst_opt[k] )\n",
    "            #add the optimum intercept\n",
    "            #self.y_hat_tst_opt[k] += self.intcpt_opt\n",
    "            #print(self.y_hat_tst_opt[k] )\n",
    "        return [self.y_hat_tst_opt]\n",
    "    \n",
    "    #generate Mean Squared Error\n",
    "    def eval_MSE(self):\n",
    "        num_rows = self.y_test.shape[0]\n",
    "        for i in range(0,num_rows):\n",
    "            self.MSE += (self.y_hat_tst_opt[i] - self.y_test[i]) ** 2\n",
    "            #print(self.y_test[i],self.y_hat_tst_opt[i] )\n",
    "        self.MSE = self.MSE / num_rows\n",
    "        \n",
    "        return [self.MSE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True)\n",
      "[[-1.31194948  0.8618651  -0.16720579  0.18959156 -1.48657697  2.79128043\n",
      "  -0.32737856 -2.77204607  2.97565149 -2.27277939 -2.13376876  1.05846715\n",
      "  -3.3349703 ]]\n",
      "[22.53714706]\n",
      "1.9403252174826322e-53\n",
      "#####MSE [array([28.70249481])]\n"
     ]
    }
   ],
   "source": [
    "#implementor code for Stocastic gradient descent USING DOT PRODUCT WITH PRECALCULATED WEIGHTS\n",
    "\n",
    "#instantiate Linear Regression object and regressor\n",
    "#object\n",
    "linregr2 = LinRegr()\n",
    "#regressor\n",
    "lin_rgrsn_2 = linregr2.linearegrsn()\n",
    "linregr2.load_data()\n",
    "\n",
    "lin_rgrsn_2 = linregr2.lnrg_fitdata()\n",
    "beta_est2 = linregr2.getCoeff()\n",
    "intercepts2 = linregr2.getintercepts()\n",
    "\n",
    "\n",
    "linregr2.learning_rate = 0.00001 \n",
    "return_list2 = linregr2.SGDProcess(100,10,linregr2.x_train.shape[1])\n",
    "\n",
    "print(return_list2[0])\n",
    "print(return_list2[1])\n",
    "print(return_list2[2])\n",
    "linregr2.eval_yhat_opt()\n",
    "MSE = linregr2.eval_MSE()\n",
    "print('#####MSE',MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error 45.694918136190324\n"
     ]
    }
   ],
   "source": [
    "#implementor code for Stocastic gradient descent USING DOT PRODUCT WITH PRECALCULATED WEIGHTS equal to Zero\n",
    "linregr3 = LinRegr()\n",
    "#regressor\n",
    "lin_rgrsn_3 = linregr3.linearegrsn()\n",
    "ret_list3 = linregr3.SGDProcess_1(100,0.194,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabulate\n",
    "res_tab = [['S No.', 'Weights','Model', 'MSE'],\n",
    "         [1,'Zeros','SGDRegressor',22.959212], \n",
    "         [2,'Pre-computed','MyModel', 28.702494],\n",
    "         [3,'Zeros', 'MyModel',45.694918]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════╤══════════════╤══════════════╤═══════════╕\n",
      "│ S No. │ Weights      │ Model        │ MSE       │\n",
      "├───────┼──────────────┼──────────────┼───────────┤\n",
      "│ 1     │ Zeros        │ SGDRegressor │ 22.959212 │\n",
      "├───────┼──────────────┼──────────────┼───────────┤\n",
      "│ 2     │ Pre-computed │ MyModel      │ 28.702494 │\n",
      "├───────┼──────────────┼──────────────┼───────────┤\n",
      "│ 3     │ Zeros        │ MyModel      │ 45.694918 │\n",
      "╘═══════╧══════════════╧══════════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "print(tabulate.tabulate(res_tab, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "06 Implement SGD.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
